# Predicting Progression of Alzheimerâ€™s Disease (PPAD)

![bi_gru_single](https://user-images.githubusercontent.com/77756538/214337034-759080e6-b94e-4add-9b33-6b19dcace437.svg)

  PPAD is a deep learning architecture based on Recurrent Neural Networks (RNN). It consists of a RNN and multi-layer perceptron (MLP). PPAD is designed for early predicting conversion from mild cognitive impairment (MCI) to Alzheimer's disease (AD) at the next visit for patients.
  
  In this architecture, the RNN component learns ğ‘¥ğ‘¡Ì‚, a latent representation of the longitudinal clinical data up to ğ‘¡ visits (Eq 1). Then, the MLP model is trained with concatenation of the cross-sectional demographic data (ğ·) and ğ‘¥ğ‘¡Ì‚ to predict conversion to AD at next visit (Eq 2).

ğ‘¥ğ‘¡Ì‚=ğ‘…ğ‘ğ‘(ğ‘‹) ...................................................(1)

ğ‘¦â€²=ğœ(ğ‘Š1(ğ‘…ğ‘’ğ¿ğ‘ˆ(ğ‘Š2(ğ‘¥ğ‘¡Ì‚âŠ•ğ·)+ğ‘2))+ğ‘1) .............................(2)

In Eq 7, ğ‘¦â€² represents the predicted diagnosis, ğ‘Š1 and ğ‘Š2 are the trainable linear transformation matrices, and ğ‘1 and ğ‘2 are the bias vectors.

To run PPAD, please have PPAD.ipynb, (dummy_lon_data_train.pkl 

# Predicting Progression of Alzheimerâ€™s Disease Autoencoder (PPAD-AE)

![bi_gru_multiple](https://user-images.githubusercontent.com/77756538/214342401-ac014f9a-9a10-46d5-b7e1-face52854f22.svg)

PPAD-AE is a deep learning architecture based on Recurrent Neural Networks (RNN). It composes of a RNN autoencoder and an MLP. PPAD-AE is designed for early predicting conversion from mild cognitive impairment (MCI) to Alzheimer's disease (AD) at multiple visits ahead for patients. 

  In this architecture, the RNN component learns a latent representation (ğ‘¥ğ‘¡Ì‚) of the longitudinal clinical data up to ğ‘¡ visits (Eq 1). Then, the latent representation is used by the decoder component to generate representations of multiple visits ahead up to ğ‘› visits. Finally, the MLP model is trained with concatenation of the cross-sectional demographic data (ğ·) and the representation of the last generated visit by the decoder to predict conversion to AD at the (ğ‘¡+ğ‘›)ğ‘¡â„ visit (Eq 3).

ğ‘¦â€²=ğœ(ğ‘Š1(ğ‘…ğ‘’ğ¿ğ‘ˆ(ğ‘Š2(ğ‘¥ğ‘¡+(ğ‘›âˆ’1)âŠ•ğ·)+ğ‘2))+ğ‘1) ........................(3)

# Parameter learning and evaluation metrics

  To increase the predictionâ€™s sensitivity for both architectures, all trainable parameters for the RNN, RNN autoencoder, and MLP were learned in an integral way using a customized binary cross-entropy loss function (Eq 4) to give more weight on predicting conversion to AD cases. We seek by using this customized loss function to minimize the false negative cases while predicting diagnosis of the future visit which leads to increased sensitivity of the predictive model.
  
ğ¿ğ‘œğ‘ ğ‘ =âˆ’1/ğ‘Î£(ğ›¼âˆ™(ğ‘¦âˆ™ğ‘™ğ‘œğ‘”ğ‘¦â€²))+((1âˆ’ğ›¼)âˆ™(1âˆ’ğ‘¦)âˆ™ğ‘™ğ‘œğ‘”(1âˆ’ğ‘¦â€²)) ................(4)

In Eq 4, ğ›¼ is a real number between 0 and 1 to define the relative weight of positive prediction, ğ‘¦ is the true diagnosis, and ğ‘¦â€² is the predicted diagnosis.

  In this study, we set ğ›¼ to 0.7. Based on the proposed customized loss function, all trainable parameters for the RNN, RNN autoencoder, and MLP (Eq 2 and 3) were updated while training models in the backpropagation. For optimization, all models were trained using Adaptive Moment Estimation (Adam) optimizer and the learning rate was set to 0.001.
  
  RNN cell, number of epochs, batch size, dropout rate, and L2 regularization are the hyperparameters that have been tuned. For model evaluation, F2 score (Eq 5) and sensitivity were used.
  
ğ¹ğ›½ = (1+ğ›½2) âˆ™ (precisionâˆ™recall) / (ğ›½2.precision+recall) .......(5)

In Eq 5, recall is considered ğ›½ times more important than precision. In this study, ğ›½ was set to 2.

# How to 
