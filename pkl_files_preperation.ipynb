{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary lib\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_string_dtype\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import pickle\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to determine unique values in a dataframe's column based on the column's name.\n",
    "def unique_value_function(df, feature_name):\n",
    "    if feature_name in df.columns:\n",
    "        _values = df[[feature_name]]\n",
    "        unique_values = _values.values\n",
    "        unique_values = np.unique(unique_values)\n",
    "        num_of_unique_values = np.unique(unique_values).shape[0]\n",
    "        #print('Number of unique values in '+feature_name+' is: ', num_of_unique_values)\n",
    "        return unique_values\n",
    "    else:\n",
    "        print(str(feature_name)+' is not a feature name in this dataframe.')\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if the dataset has all necessary features and been preprocessed carefully.\n",
    "def check_longitudinal_dataset(df):\n",
    "    # chech if the dataframe is empty\n",
    "    if df.empty:\n",
    "        print('Dataset is empty.')\n",
    "        return -1\n",
    "    \n",
    "    # Check for missing values\n",
    "    if df.isnull().sum().any():\n",
    "        print('Dataset has NAN values and needs to preprocess.')\n",
    "        return -1\n",
    "    \n",
    "    # Check for the existence of important features\n",
    "    features_name = df.columns\n",
    "    if not('RID' in features_name) or not('VISCODE' in features_name) or not('DX' in features_name):\n",
    "        print('Dataset does not have necessary feature/s')\n",
    "        return -1\n",
    "    \n",
    "    # chech if the RID's is numeric\n",
    "    if not(is_numeric_dtype(df['RID'])):\n",
    "        print('Patient ID should be numeric.')\n",
    "        return -1\n",
    "    \n",
    "    # Check for the existence of only Dementia and MCI as diagnosis\n",
    "    unique_diagnosis = unique_value_function(df, 'DX')\n",
    "    if (len(unique_diagnosis) != 2) or not('Dementia' in unique_diagnosis) or not('MCI' in unique_diagnosis):\n",
    "        print('Dataset does not have correct diagnosis or unique number of diagnosis.')\n",
    "        return -1\n",
    "    \n",
    "    # Check if the dataframe has at least one longitudinal feature other than RID, VISCODE, and DX\n",
    "    if not(df.shape[1] > 3):\n",
    "        print('Dataset does not have enough features')\n",
    "        return -1\n",
    "    \n",
    "    # Check if the longitudinal data is numiric\n",
    "    features_name = df.columns\n",
    "    flag = False\n",
    "    for i in range(len(features_name)):\n",
    "        if not(features_name[i] in ['RID', 'DX', 'VISCODE']):\n",
    "            if not(is_numeric_dtype(df[features_name[i]])):\n",
    "                flag = True\n",
    "    if flag:\n",
    "        print('Data should be numeric.')\n",
    "        return -1\n",
    "            \n",
    "    \n",
    "    # return 1 if the dataset is ready\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if the dataset has all necessary features and been preprocessed carefully.\n",
    "def check_demographic_dataset(df):\n",
    "    # chech if the dataframe is empty\n",
    "    if df.empty:\n",
    "        print('Dataset is empty.')\n",
    "        return -1\n",
    "    \n",
    "    # Check for missing values\n",
    "    if df.isnull().sum().any():\n",
    "        print('Dataset has NAN values and needs to preprocess.')\n",
    "        return -1\n",
    "    \n",
    "    # Check for the existence of important features\n",
    "    features_name = df.columns\n",
    "    if not('RID' in features_name):\n",
    "        print('Dataset does not have necessary feature/s')\n",
    "        return -1\n",
    "    \n",
    "    # chech if the RID's is numeric\n",
    "    if not(is_numeric_dtype(df['RID'])):\n",
    "        print('Patient ID should be numeric.')\n",
    "        return -1\n",
    "    \n",
    "    # Check if the dataframe has at least one demographic feature other than RID\n",
    "    if not(df.shape[1] > 1):\n",
    "        print('Dataset does not have enough features')\n",
    "        return -1\n",
    "    \n",
    "    # return 1 if the dataset is ready\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare longitudinal data (VISCODE)\n",
    "def visit_code_preperation(df):\n",
    "    unique_visitcode = unique_value_function(df, 'VISCODE')\n",
    "    unique_id = unique_value_function(df, 'RID')\n",
    "    \n",
    "    columns_name = list(df.columns)\n",
    "    new_df = pd.DataFrame(columns = columns_name)\n",
    "    \n",
    "    for i in range(len(unique_id)):\n",
    "        temp_data = df[df[\"RID\"] == unique_id[i]]\n",
    "        temp_data.reset_index(drop=True,inplace=True)\n",
    "        size = temp_data.shape[0]\n",
    "        \n",
    "        for j in range(size):\n",
    "            temp_data.loc[j, 'VISCODE'] = j*6\n",
    "            new_row = temp_data.iloc[j,:]\n",
    "            new_df.loc[len(new_df)] = new_row\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_patients_visits_function(df, _list):\n",
    "    # Calculate maximum number of visits\n",
    "    visit_size = 0\n",
    "    for i in range(len(_list)):\n",
    "        temp_data = df[df[\"RID\"] == _list[i]]\n",
    "        size = (temp_data.shape)[0]\n",
    "        if size > visit_size:\n",
    "            visit_size = size\n",
    "\n",
    "    # Calculate how many patients in each number of visit groups for removed patients\n",
    "    visit_size = np.zeros((visit_size),int)\n",
    "    for i in range(len(_list)):\n",
    "        temp_data = df[df[\"RID\"] == _list[i]]\n",
    "        size = (temp_data.shape)[0]\n",
    "        visit_size[size-1] = visit_size[size-1] + 1\n",
    "\n",
    "    for i in range(len(visit_size)):\n",
    "        print (i+1,'_Visit = ', visit_size[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Diagnosis DX (MCI = 0 and Dementia = 1)\n",
    "def encode_diagnosis(df):\n",
    "    for i in range(len(df)):\n",
    "        if df.loc[i, 'DX'] == 'MCI':\n",
    "            df.loc[i, 'DX'] = 0\n",
    "        else:\n",
    "            df.loc[i, 'DX'] = 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize longitudinal data using min-max normalization\n",
    "def min_max_normalization(df):\n",
    "    columns = list(df.columns)\n",
    "    new_arrangment_for_columns = ['RID','VISCODE']\n",
    "    for col in columns:\n",
    "        if not(col in new_arrangment_for_columns) and col != 'DX':\n",
    "            new_arrangment_for_columns.append(col)\n",
    "    new_arrangment_for_columns.append('DX')\n",
    "    \n",
    "    df = df[new_arrangment_for_columns]\n",
    "    \n",
    "    for i in range(2, len(new_arrangment_for_columns)-1):   \n",
    "        temp_data = df.iloc[:,i]\n",
    "        max_value = temp_data.max()\n",
    "        min_value = temp_data.min()\n",
    "        for j in range(len(df)):\n",
    "            df.iat[j, i] = (df.iloc[j, i]-min_value)/(max_value - min_value)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to group patients together based on number of visits they have\n",
    "def group_patients_according_number_of_visits(df):\n",
    "    unique_id = unique_value_function(df, 'RID')\n",
    "    visits_dic = {}\n",
    "    \n",
    "    for i in range(len(unique_id)):\n",
    "        temp_data = df[df[\"RID\"] == unique_id[i]]\n",
    "        temp_data.reset_index(drop=True,inplace=True)\n",
    "        size = temp_data.shape[0]\n",
    "        \n",
    "        if size in visits_dic:\n",
    "            visits_dic[size] = pd.concat([visits_dic[size], temp_data])\n",
    "            visits_dic[size].reset_index(drop=True, inplace=True)\n",
    "        else:\n",
    "            visits_dic[size] = temp_data\n",
    "            \n",
    "    # sort the dictionary based on the key\n",
    "    sorted_dic = {}\n",
    "    for key in sorted(visits_dic):\n",
    "        sorted_dic[key] = visits_dic[key]\n",
    "    return sorted_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transpose the longitudinal dataset\n",
    "def transpose_longitudinal_data(group_longitudinal_data_dic, features_to_be_in_columns):\n",
    "    transposed_lonitudinal_data_dic = {}\n",
    "    for key in (group_longitudinal_data_dic):\n",
    "        transposed_lonitudinal_data_dic[key] = group_longitudinal_data_dic[key].pivot(index = 'RID', columns= 'VISCODE',\n",
    "                                                                                      values= features_to_be_in_columns)\n",
    "        \n",
    "    new_columns_names_dic = {}\n",
    "    for key in (group_longitudinal_data_dic):\n",
    "        new_columns_names_dic[key] = ['RID']\n",
    "    for key in (group_longitudinal_data_dic):\n",
    "        time_points = key\n",
    "        \n",
    "        for i in range(time_points):\n",
    "            for j in range(1, int(len(transposed_lonitudinal_data_dic[key].columns)/time_points+1)):\n",
    "                column_idex = i + (key * j) - key\n",
    "                new_columns_names_dic[key].append(transposed_lonitudinal_data_dic[key].columns[column_idex][0] + '_'+ str(transposed_lonitudinal_data_dic[key].columns[column_idex][1]))\n",
    "                \n",
    "    final_longitudinal_data_dic = {}\n",
    "    for key in (group_longitudinal_data_dic):\n",
    "        time_points = key\n",
    "        unique_rid = unique_value_function(group_longitudinal_data_dic[key], 'RID')\n",
    "\n",
    "        final_longitudinal_data_dic[key] = pd.DataFrame(columns = new_columns_names_dic[key])\n",
    "        for x in range(len(transposed_lonitudinal_data_dic[key])):\n",
    "            new_time_point_data = []\n",
    "            new_time_point_data.append(unique_rid[x])\n",
    "            for i in range(time_points):\n",
    "                for j in range(1, int(len(transposed_lonitudinal_data_dic[key].columns)/time_points+1)):\n",
    "                    column_idex = i + (time_points * j) - time_points\n",
    "                    new_time_point_data.append(transposed_lonitudinal_data_dic[key].iloc[x, column_idex])\n",
    "            final_longitudinal_data_dic[key].loc[len(final_longitudinal_data_dic[key])] = new_time_point_data\n",
    "            \n",
    "    return final_longitudinal_data_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply one-hot-encoding only for categorical demographics features\n",
    "def demographic_one_hot_encoding(demographic_df):\n",
    "    \n",
    "    demographic_data = demographic_df\n",
    "    categorical_columns = []\n",
    "    all_columns = list(demographic_data.columns)\n",
    "    for i in range(len(all_columns)):\n",
    "        if all_columns[i] != 'RID' and all_columns[i] != 'PTEDUCAT':\n",
    "            categorical_columns.append(all_columns[i])\n",
    "\n",
    "    for c in range(len(categorical_columns)):\n",
    "        tempdf = pd.get_dummies(demographic_data[categorical_columns[c]], prefix=categorical_columns[c])\n",
    "        demographic_data = pd.concat([demographic_data, tempdf], axis=1)\n",
    "        demographic_data = demographic_data.drop(columns=categorical_columns[c])\n",
    "\n",
    "    categorical_columns_will_be_used = list(demographic_data.columns)\n",
    "\n",
    "    temp_columns = demographic_data.columns\n",
    "    temp_keep_these_columns = []\n",
    "    for c in range(len(temp_columns)):\n",
    "        for k in range(len(categorical_columns_will_be_used)):\n",
    "            if categorical_columns_will_be_used[k] in temp_columns[c]:\n",
    "                temp_keep_these_columns.append(temp_columns[c])\n",
    "    demographic_data = demographic_data[temp_keep_these_columns]\n",
    "    \n",
    "    return demographic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split the longitudinal data into training and test data 70% and 30% respectively\n",
    "def split_longitudinal_data(longitudinal_data_dic):\n",
    "    train_data = {}\n",
    "    test_data = {}\n",
    "    \n",
    "    for key in longitudinal_data_dic:\n",
    "        X_train, X_test = train_test_split(longitudinal_data_dic[key], test_size=0.3, random_state=42)\n",
    "        if key in train_data:\n",
    "            train_data[key] = pd.concat([train_data[key], X_train])\n",
    "            train_data[key].reset_index(drop=True, inplace=True)\n",
    "            \n",
    "            test_data[key] = pd.concat([test_data[key], X_test])\n",
    "            test_data[key].reset_index(drop=True, inplace=True)\n",
    "        else:\n",
    "            train_data[key] = X_train\n",
    "            test_data[key] = X_test\n",
    "            \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global veriable\n",
    "num_features_in_each_time_step = 1\n",
    "time_steps = 2\n",
    "demographic_features = 1\n",
    "\n",
    "# Training data lists\n",
    "dataset = []\n",
    "demographic_train = []\n",
    "target_1 = []\n",
    "\n",
    "# Test data lists\n",
    "Testset = []\n",
    "demographic_test = []\n",
    "target_2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create training lists (longitudinal, demographics, label)\n",
    "def create_train_lists(longitudinal_df, demographic_df, tp, ftp):\n",
    "    global dataset\n",
    "    global demographic_train\n",
    "    global target_1\n",
    "    \n",
    "    uid = unique_value_function(longitudinal_df, 'RID')\n",
    "    temp_demographic_df = pd.DataFrame(columns = list(demographic_df.columns)[1::])\n",
    "    for i in range(len(uid)):\n",
    "        temp_data = demographic_df[demographic_df[\"RID\"] == uid[i]]\n",
    "        temp_data.reset_index(drop=True, inplace=True)\n",
    "        new_row = temp_data.iloc[0,1:]\n",
    "        temp_demographic_df.loc[len(temp_demographic_df)] = new_row\n",
    "    \n",
    "    \n",
    "    num_feature_in_tp = num_features_in_each_time_step\n",
    "    df1 = longitudinal_df[longitudinal_df.columns]\n",
    "    \n",
    "    diagnosis_columns_names = []\n",
    "    all_columns = list(longitudinal_df.columns)\n",
    "    for i in range(len(all_columns)):\n",
    "        if 'DX_' in all_columns[i]:\n",
    "            diagnosis_columns_names.append(all_columns[i])\n",
    "    \n",
    "    # dataframe at least has one time point for data and ftp for prediction \n",
    "    if (df1.shape[1] - 1) / (num_feature_in_tp + 1) >= ftp+1:\n",
    "        Features = df1.loc[:, ~df1.columns.isin(diagnosis_columns_names)]\n",
    "        \n",
    "        Labels = df1.loc[:, df1.columns.isin(diagnosis_columns_names)]\n",
    "\n",
    "                    \n",
    "        # dataframe has tp and ftp\n",
    "        if (df1.shape[1] - 1) / (num_feature_in_tp + 1) >= tp+ftp:\n",
    "            for i in range(len(df1)):\n",
    "                dataset.append(list(Features.iloc[i,1:tp*num_feature_in_tp+1]))\n",
    "                demographic_train.append(list(temp_demographic_df.iloc[i,:]))\n",
    "                target_1.append(list(Labels.iloc[i,tp:tp+ftp]))\n",
    "        else:\n",
    "            for i in range(len(df1)):\n",
    "                dataset.append(list(Features.iloc[i,1:Features.shape[1] - (ftp*num_feature_in_tp)]))\n",
    "                demographic_train.append(list(temp_demographic_df.iloc[i,:]))\n",
    "                target_1.append(list(Labels.iloc[i,Labels.shape[1]-ftp:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create test lists (longitudinal, demographics, label)\n",
    "def create_test_lists(longitudinal_df, demographic_df, tp, ftp):\n",
    "    global Testset\n",
    "    global target_2\n",
    "    global demographic_test\n",
    "    global target_2_prev\n",
    "    \n",
    "    uid = unique_value_function(longitudinal_df, 'RID')\n",
    "    temp_demographic_df = pd.DataFrame(columns = list(demographic_df.columns)[1::])\n",
    "    for i in range(len(uid)):\n",
    "        temp_data = demographic_df[demographic_df[\"RID\"] == uid[i]]\n",
    "        temp_data.reset_index(drop=True, inplace=True)\n",
    "        new_row = temp_data.iloc[0,1:]\n",
    "        temp_demographic_df.loc[len(temp_demographic_df)] = new_row\n",
    "    \n",
    "    \n",
    "    num_feature_in_tp = num_features_in_each_time_step\n",
    "    df1 = longitudinal_df[longitudinal_df.columns]\n",
    "    \n",
    "    diagnosis_columns_names = []\n",
    "    all_columns = list(longitudinal_df.columns)\n",
    "    for i in range(len(all_columns)):\n",
    "        if 'DX_' in all_columns[i]:\n",
    "            diagnosis_columns_names.append(all_columns[i])\n",
    "    \n",
    "    # dataframe must have tp+ftp visits \n",
    "    if (df1.shape[1] - 1) / (num_feature_in_tp + 1) >= tp+ftp:\n",
    "        Features = df1.loc[:, ~df1.columns.isin(diagnosis_columns_names)]\n",
    "        \n",
    "        Labels = df1.loc[:, df1.columns.isin(diagnosis_columns_names)]\n",
    "\n",
    "        for i in range(len(df1)):\n",
    "            Testset.append(list(Features.iloc[i,1:tp*num_feature_in_tp+1]))\n",
    "            demographic_test.append(list(temp_demographic_df.iloc[i,:]))\n",
    "            target_2.append(list(Labels.iloc[i,tp:tp+ftp]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the train dataset\n",
    "def create_dataset_train(train_data_list, ts, fts, demographic_df):\n",
    "    global time_steps\n",
    "    global demographic_features\n",
    "    global dataset\n",
    "    global demographic_train\n",
    "    global target_1\n",
    "    \n",
    "    dataset = []\n",
    "    demographic_train = []\n",
    "    target_1 = []\n",
    "\n",
    "    time_steps = ts\n",
    "    \n",
    "    train_df_list = []\n",
    "    \n",
    "    for i in range(len(train_data_list)):\n",
    "        train_df_list.append(train_data_list[i])\n",
    "    \n",
    "    #create_train_lists(df1_train,time_steps)\n",
    "    for i in range(len(train_df_list)):\n",
    "        create_train_lists(train_df_list[i], demographic_df, time_steps, fts)\n",
    "        \n",
    "    # Train Padding\n",
    "    padded1 = pad_sequences(dataset, padding='post',dtype='float', value=-1)\n",
    "\n",
    "    num_samples = len(padded1)\n",
    "    num_features = padded1.shape[1]\n",
    "    time_steps = int(num_features / num_features_in_each_time_step)\n",
    "    dataset = padded1\n",
    "    padded_ = pad_sequences(target_1, padding='post',dtype='float', value=-1)\n",
    "    target_1 = padded_\n",
    "    num_labels = padded_.shape[1]\n",
    "    # data and target are reshaped into the 3D format expected by LSTMs, namely [samples, timesteps, features].\n",
    "    dataset = np.reshape(dataset, (num_samples, time_steps, num_features_in_each_time_step))\n",
    "    target_1 = np.reshape(target_1, (num_samples, num_labels, 1))\n",
    "\n",
    "    \n",
    "    return dataset, target_1, demographic_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create test dataset\n",
    "def create_dataset_test(test_data_list, ts, fts, demographic_df):\n",
    "    global time_steps\n",
    "    global demographic_features\n",
    "    global Testset\n",
    "    global target_2\n",
    "    global demographic_test\n",
    "    global target_2_prev\n",
    "    \n",
    "    Testset = []\n",
    "    target_2 = []\n",
    "    demographic_test = []\n",
    "    target_2_prev = []\n",
    "\n",
    "    time_steps = ts\n",
    "    \n",
    "    test_df_list = []\n",
    "    \n",
    "    for i in range(len(test_data_list)):\n",
    "        test_df_list.append(test_data_list[i])\n",
    "    \n",
    "    #create_train_lists(df1_train,time_steps)\n",
    "    for i in range(len(test_df_list)):\n",
    "        create_test_lists(test_df_list[i], demographic_df, time_steps, fts)\n",
    "        \n",
    "    # Test Padding\n",
    "    padded2 = pad_sequences(Testset, padding='post',dtype='float', value=-1)\n",
    "\n",
    "    T_num_samples = len(padded2)\n",
    "    Testset = padded2\n",
    "    target_2 = np.array(target_2)\n",
    "\n",
    "    # Test data and target are reshaped into the 3D format expected by LSTMs, namely [samples, timesteps, features].\n",
    "    Testset = np.reshape(Testset, (T_num_samples, time_steps, num_features_in_each_time_step))\n",
    "    target_2 = np.reshape(target_2, (T_num_samples, fts, 1))\n",
    "\n",
    "    \n",
    "    return Testset, target_2, demographic_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function\n",
    "def pkl_files_creator():\n",
    "    global num_features_in_each_time_step\n",
    "    global time_steps\n",
    "    global demographic_features\n",
    "    # Read csv files for longitudinal and demographic data\n",
    "    # Longitudinal data\n",
    "    file_name = 'longitudinal_data.csv'\n",
    "    longitudinal_df = read_csv(file_name, header=0)\n",
    "\n",
    "    # Demographic data\n",
    "    file_name = 'demographic_data.csv'\n",
    "    demographic_df = read_csv(file_name, header=0)\n",
    "    \n",
    "    # working on longitudinal data\n",
    "    if check_longitudinal_dataset(longitudinal_df) == -1:\n",
    "        return -1\n",
    "    if check_demographic_dataset(demographic_df) == -1:\n",
    "        return -1\n",
    "    longitudinal_df = visit_code_preperation(longitudinal_df)\n",
    "    longitudinal_df = encode_diagnosis(longitudinal_df)\n",
    "    longitudinal_df = min_max_normalization(longitudinal_df)\n",
    "    longitudinal_df_dic = group_patients_according_number_of_visits(longitudinal_df)\n",
    "    features_to_be_in_columns = 0\n",
    "    for key in longitudinal_df_dic:\n",
    "        features_to_be_in_columns = list(longitudinal_df_dic[key].columns)[2::]\n",
    "        break\n",
    "    longitudinal_df_dic = transpose_longitudinal_data(longitudinal_df_dic, features_to_be_in_columns)\n",
    "    longitudinal_train_data, longitudinal_test_data = split_longitudinal_data(longitudinal_df_dic)\n",
    "    \n",
    "    # working on demographic data\n",
    "    demographic_df = demographic_one_hot_encoding(demographic_df)\n",
    "    \n",
    "    # user choises\n",
    "    number_of_training_visits = input(\"Please enter number of visits that you want to use for training the model:\\n\")\n",
    "    while not(number_of_training_visits.isdigit()):\n",
    "        number_of_training_visits = input(\"Please enter integer values:\\n\")\n",
    "    number_of_training_visits = int(number_of_training_visits)\n",
    "    \n",
    "    number_of_future_visits = input(\"Please enter number of future visits that you want to predict:\\n\")\n",
    "    while not(number_of_future_visits.isdigit()):\n",
    "        number_of_future_visits = input(\"Please enter integer values:\\n\")\n",
    "    number_of_future_visits = int(number_of_future_visits)\n",
    "    \n",
    "    key_list = []\n",
    "    for key in longitudinal_df_dic:\n",
    "        key_list.append(key)\n",
    "    minimum_visit = key_list[0]\n",
    "    maximum_visit = key_list[-1]\n",
    "    if (number_of_future_visits + number_of_training_visits) > maximum_visit:\n",
    "        print('The Dataset does not have enough visits for this selection')\n",
    "        return -1\n",
    "    if number_of_training_visits < minimum_visit or number_of_future_visits < 1:\n",
    "        print('Wrong selection')\n",
    "        return -1\n",
    "    \n",
    "    num_features_in_each_time_step = longitudinal_df.shape[1] - 3\n",
    "    time_steps = 0\n",
    "    demographic_features = demographic_df.shape[1] - 1\n",
    "    \n",
    "    # train longitudinal data\n",
    "    lon_train_data_list = []\n",
    "    ###############################\n",
    "    # train demographics data\n",
    "    dem_train_data_list = []\n",
    "    ###############################\n",
    "    #test longitudinal data\n",
    "    lon_test_data_list = []\n",
    "    ###################################\n",
    "    #test demographic data\n",
    "    dem_test_data_list = []\n",
    "    ###################################\n",
    "    train_label_list = []\n",
    "    ###################################\n",
    "    test_label_list = []\n",
    "    \n",
    "    train = []\n",
    "    test = []\n",
    "    for key in longitudinal_train_data:\n",
    "        train.append(longitudinal_train_data[key])\n",
    "    for key in longitudinal_test_data:\n",
    "        test.append(longitudinal_test_data[key])\n",
    "        \n",
    "    X_train, y_train, demographic_train_data = create_dataset_train(train, number_of_training_visits,number_of_future_visits,\n",
    "                                                                    demographic_df)\n",
    "    # train data\n",
    "    lon_train_data_list.append(X_train)\n",
    "    dem_train_data_list.append(demographic_train_data)\n",
    "    train_label_list.append(y_train)\n",
    "    \n",
    "    X_test, y_test, demographic_test_data = create_dataset_test(test, number_of_training_visits, number_of_future_visits,\n",
    "                                                                demographic_df)\n",
    "    # test data\n",
    "    lon_test_data_list.append(X_test)\n",
    "    dem_test_data_list.append(demographic_test_data)\n",
    "    test_label_list.append(y_test)\n",
    "        \n",
    "    f = open('longitudinal_data_train.pkl', 'wb')\n",
    "    pickle.dump(lon_train_data_list, f)\n",
    "    f.close()\n",
    "    f = open('label_train.pkl', 'wb')\n",
    "    pickle.dump(train_label_list, f)\n",
    "    f.close()\n",
    "    f = open('demographic_data_train.pkl', 'wb')\n",
    "    pickle.dump(dem_train_data_list, f)\n",
    "    f.close()\n",
    "    f = open('longitudinal_data_test.pkl', 'wb')\n",
    "    pickle.dump(lon_test_data_list, f)\n",
    "    f.close()\n",
    "    f = open('label_test.pkl', 'wb')\n",
    "    pickle.dump(test_label_list, f)\n",
    "    f.close()\n",
    "    f = open('demographic_data_test.pkl', 'wb')\n",
    "    pickle.dump(dem_test_data_list, f)\n",
    "    f.close()\n",
    "    return 0   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To call helping functions and generate pkl files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maa0664.UNT\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  uniques = Index(uniques)\n",
      "C:\\Users\\maa0664.UNT\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  uniques = Index(uniques)\n",
      "C:\\Users\\maa0664.UNT\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  uniques = Index(uniques)\n",
      "C:\\Users\\maa0664.UNT\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  uniques = Index(uniques)\n",
      "C:\\Users\\maa0664.UNT\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  uniques = Index(uniques)\n",
      "C:\\Users\\maa0664.UNT\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  uniques = Index(uniques)\n",
      "C:\\Users\\maa0664.UNT\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  uniques = Index(uniques)\n",
      "C:\\Users\\maa0664.UNT\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  uniques = Index(uniques)\n",
      "C:\\Users\\maa0664.UNT\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  uniques = Index(uniques)\n",
      "C:\\Users\\maa0664.UNT\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  uniques = Index(uniques)\n",
      "C:\\Users\\maa0664.UNT\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  uniques = Index(uniques)\n",
      "C:\\Users\\maa0664.UNT\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  uniques = Index(uniques)\n",
      "C:\\Users\\maa0664.UNT\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  uniques = Index(uniques)\n",
      "C:\\Users\\maa0664.UNT\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  uniques = Index(uniques)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter number of visits that you want to use for training the model:\n",
      "2\n",
      "Please enter number of future visits that you want to predict:\n",
      "2\n",
      "Data is ready as pkl files.\n"
     ]
    }
   ],
   "source": [
    "if pkl_files_creator() == -1:\n",
    "    print('There is an error! Please run it again.')\n",
    "else:\n",
    "    print('Data is ready as pkl files.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
